<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>FutureTech | AI Demos</title>
  <style>
    body {
      background: #0d0d0d;
      color: white;
      font-family: 'Segoe UI', sans-serif;
      margin: 0;
      padding: 0;
    }
    .container {
      padding: 60px 20px;
      text-align: center;
    }
    h1 {
      font-size: 3rem;
      margin-bottom: 10px;
      color: #00ffff;
    }
    p {
      color: #ccc;
      max-width: 600px;
      margin: 0 auto 40px;
    }
    .cards {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 30px;
    }
    .card {
      background: #1a1a1a;
      border: 1px solid #2e2e2e;
      border-radius: 15px;
      padding: 30px;
      width: 320px;
      transition: transform 0.3s ease;
      text-align: left;
    }
    .card:hover {
      transform: scale(1.05);
    }
    .card h3 {
      color: #00ffff;
      margin-bottom: 15px;
    }
    .card p, label {
      font-size: 0.95rem;
      color: #bbb;
    }
    textarea, input[type="file"] {
      width: 100%;
      padding: 10px;
      border-radius: 8px;
      margin-bottom: 10px;
    }
    .btn {
      background: #00ffff;
      color: #000;
      border: none;
      padding: 10px 20px;
      font-weight: bold;
      border-radius: 10px;
      cursor: pointer;
    }
    .btn:hover {
      background: #00cccc;
    }
    video, canvas, img {
      width: 100%;
      margin-top: 10px;
      border-radius: 10px;
    }
  </style>
  <script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.20.0"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api/dist/face-api.min.js"></script>
</head>
<body>
  <div class="container">
    <h1>AI in Action üöÄ</h1>
    <p>Experience AI live in your browser ‚Äî powered by the latest web technology.</p>

    <div class="cards">
      <!-- AI Text Generator -->
      <div class="card">
        <h3>üß† AI Text Generator</h3>
        <textarea id="userPrompt" rows="4" placeholder="Type your idea or topic here..."></textarea>
        <button onclick="generateText()" class="btn">Generate</button>
        <button onclick="startSpeech()" class="btn" style="margin-left: 10px;">üé§ Speak</button>
        <p id="outputText" style="margin-top:1rem; color:#99ffee;">Your AI-generated text will appear here...</p>
      </div>

      <!-- Face Detection -->
      <div class="card">
        <h3>üòé Face Detection</h3>
        <button onclick="startFace()" class="btn">Start Camera</button>
        <video id="video" autoplay muted></video>
        <canvas id="overlay"></canvas>
      </div>

      <!-- Voice to Text -->
      <div class="card">
        <h3>üé§ Voice to Text</h3>
        <button onclick="startSpeech()" class="btn">Start Speaking</button>
        <p id="voiceText" style="margin-top:1rem; color:#99ffee;">Your speech will appear here...</p>
      </div>

      <!-- Image Recognition -->
      <div class="card">
        <h3>üñºÔ∏è Image Recognition</h3>
        <input type="file" accept="image/*" onchange="recognizeImage(event)" />
        <img id="inputImage" />
        <p id="imageResult" style="margin-top:1rem; color:#99ffee;"></p>
      </div>
    </div>
  </div>

  <script>
    // AI Text Generator using distilgpt2
    async function generateText() {
      const prompt = document.getElementById("userPrompt").value;
      document.getElementById("outputText").innerText = "Generating...";

      const response = await fetch("https://api-inference.huggingface.co/models/distilgpt2", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": "Bearer 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"
        },
        body: JSON.stringify({ inputs: prompt })
      });

      try {
        const data = await response.json();
        document.getElementById("outputText").innerText =
          data[0]?.generated_text || "The model is still warming up. Please try again.";
      } catch (err) {
        document.getElementById("outputText").innerText = "An error occurred. Please try again.";
      }
    }

    // Face Detection
    async function startFace() {
      const video = document.getElementById('video');
      const canvas = document.getElementById('overlay');
      const displaySize = { width: 320, height: 240 };

      await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model/');
      navigator.mediaDevices.getUserMedia({ video: {} }).then(stream => {
        video.srcObject = stream;
      });

      video.addEventListener('play', () => {
        canvas.width = video.width = displaySize.width;
        canvas.height = video.height = displaySize.height;
        setInterval(async () => {
          const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
          const resized = faceapi.resizeResults(detections, displaySize);
          canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
          faceapi.draw.drawDetections(canvas, resized);
        }, 100);
      });
    }

    // Voice Recognition
    function startSpeech() {
      const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.lang = 'en-US';
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      document.getElementById("voiceText").innerText = "üé§ Listening...";

      recognition.onresult = function(event) {
        const transcript = event.results[0][0].transcript;
        document.getElementById("voiceText").innerText = transcript;
        document.getElementById("userPrompt").value = transcript; // Optional: feed into text box
      };

      recognition.onerror = function(event) {
        document.getElementById("voiceText").innerText = "‚ö†Ô∏è Error: " + event.error;
      };

      recognition.onend = function() {
        console.log("Speech recognition ended");
      };

      recognition.start();
    }

    // Image Recognition
    async function recognizeImage(event) {
      const imageElement = document.getElementById("inputImage");
      imageElement.src = URL.createObjectURL(event.target.files[0]);

      const model = await mobilenet.load();
      const predictions = await model.classify(imageElement);
      document.getElementById("imageResult").innerText =
        predictions.map(p => `${p.className} (${(p.probability * 100).toFixed(1)}%)`).join(", ");
    }
  </script>
</body>
</html>
